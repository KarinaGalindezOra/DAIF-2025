{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf511e6b-8b91-4b99-aeec-fd460c5fd68f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.63.3\n"
     ]
    }
   ],
   "source": [
    "!oci --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a503bec-6da6-4523-9426-0d88c2c4851c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"qué es llama?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90590031-1848-487a-b751-74859289a529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"response\": \"\\\"Llama\\\" puede tener diferentes significados dependiendo del contexto. Aquí te explico los más comunes:\\n\\n1. **Animal**: La llama es un mamífero camélido originario de Sudamérica, especialmente de los Andes (Perú, Bolivia, Chile, Argentina). Es un animal doméstico utilizado principalmente como animal de carga y por su lana. Es pariente de la alpaca, el guanaco y la vicuña. Las llamas son conocidas por su temperamento tranquilo, aunque pueden escupir si se sienten amenazadas o molestas.\\n\\n2. **Fuego o flama**: En español, \\\"llama\\\" también significa \\\"flama\\\" o la parte visible del fuego. Por ejemplo: \\\"La llama de la vela es muy brillante\\\".\\n\\n3. **Llamada telefónica (en algunos países)**: En ciertos países de habla hispana, como México, \\\"llama\\\" puede ser una forma coloquial de referirse a una llamada telefónica. Por ejemplo: \\\"¿Me haces una llama cuando llegues?\\\"\\n\\n4. **Llama como verbo**: Es la tercera persona del singular del verbo \\\"llamar\\\" en presente. Por ejemplo: \\\"Él llama a su amigo todos los días\\\".\\n\\n5. **LLaMA (en tecnología)**: Si te refieres a algo relacionado con tecnología, LLaMA es el nombre de un modelo de inteligencia artificial desarrollado por Meta (la empresa detrás de Facebook). Significa \\\"Large Language Model Meta AI\\\" y se utiliza para tareas de procesamiento de lenguaje natural.\\n\\nSi tienes un contexto más específico, puedo profundizar en alguno de estos significados. ¿A qué te refieres exactamente?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import oci\n",
    "import json\n",
    "from oci.auth.signers import get_resource_principals_signer\n",
    "\n",
    "# === Config ===\n",
    "REGION = \"us-chicago-1\"\n",
    "SERVICE_ENDPOINT = f\"https://inference.generativeai.{REGION}.oci.oraclecloud.com\"\n",
    "COMPARTMENT_ID = \"ocid1.compartment.oc1..aaaaaaaawy5gpq67r3rxc7yf6yn7oesvvgr56dqg7o5y3f5ndzt2mddtj46a\"\n",
    "MODEL_ID = \"ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceya6dvgvvj3ovy4lerdl6fvx525x3yweacnrgn4ryfwwcoq\"\n",
    "\n",
    "# === Signer===\n",
    "signer = get_resource_principals_signer()\n",
    "cfg = {\"region\": REGION}\n",
    "\n",
    "# (opcional)\n",
    "if MODEL_ID is None:\n",
    "    from oci.generative_ai import GenerativeAiClient\n",
    "    genai = GenerativeAiClient(config=cfg, signer=signer)\n",
    "    models = genai.list_models(\n",
    "        compartment_id=COMPARTMENT_ID,\n",
    "        capability=[\"CHAT\"],\n",
    "        lifecycle_state=\"ACTIVE\"\n",
    "    ).data.items\n",
    "    assert models, \"No hay modelos CHAT visibles en el compartimento. Revisa permisos/compartimento.\"\n",
    "    MODEL_ID = models[0].id\n",
    "    print(\"Usando modelo:\", MODEL_ID)\n",
    "\n",
    "# === Cliente de inferencia ===\n",
    "inf = oci.generative_ai_inference.GenerativeAiInferenceClient(\n",
    "    config=cfg, signer=signer, service_endpoint=SERVICE_ENDPOINT\n",
    ")\n",
    "\n",
    "# === Prompt del usuario ===\n",
    "user_input = user_input\n",
    "\n",
    "# --- Construcción del request ---\n",
    "content = oci.generative_ai_inference.models.TextContent(text=user_input)\n",
    "message = oci.generative_ai_inference.models.Message(role=\"USER\", content=[content])\n",
    "\n",
    "chat_request = oci.generative_ai_inference.models.GenericChatRequest(\n",
    "    api_format=oci.generative_ai_inference.models.BaseChatRequest.API_FORMAT_GENERIC,\n",
    "    messages=[message],\n",
    "    max_tokens=600,\n",
    "    temperature=1.0,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    top_p=0.75,\n",
    ")\n",
    "\n",
    "chat_detail = oci.generative_ai_inference.models.ChatDetails(\n",
    "    serving_mode=oci.generative_ai_inference.models.OnDemandServingMode(model_id=MODEL_ID),\n",
    "    chat_request=chat_request,\n",
    "    compartment_id=COMPARTMENT_ID,\n",
    ")\n",
    "\n",
    "# === Llamada ===\n",
    "resp = inf.chat(chat_detail)\n",
    "\n",
    "# === Resultado ===\n",
    "choices = resp.data.chat_response.choices\n",
    "response_text = choices[0].message.content[0].text if choices else \"No se generó respuesta.\"\n",
    "print(json.dumps({\"response\": response_text}, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ec1c7-a765-4dc9-99bb-b958d9b314ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
